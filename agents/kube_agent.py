import os
from typing import Optional
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_react_agent, AgentExecutor, create_openai_tools_agent
from langchain.memory import ConversationBufferMemory, ConversationTokenBufferMemory
from langchain_community.chat_message_histories import RedisChatMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain.schema import StrOutputParser
import dotenv

from agents.kube_prompt import REACT_PROMPT
from tools import *
from tools.fortune_tools import search, bazi_cesuan, yaoyigua, jiemeng

# Load environment variables
dotenv.load_dotenv()

# Redis URL for chat history
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# ç®—å‘½å·¥å…·é–‹é—œï¼ˆé è¨­é—œé–‰ï¼‰
ENABLE_FORTUNE_TOOLS = os.getenv("ENABLE_FORTUNE_TOOLS", "false").lower() == "true"

class KubeAgent:
    """KubeAgent is an AI agent that helps users with Kubernetes issues.

    Init args â€” completion params:
    - llm: llm model
    - user_id: unique user identifier for Redis chat history
    - debug_level: debug level for the agent, 0 is no debug, 1 is verbose, 2 is verbose with intermediate steps
    """

    name: str = "KubeAgent"

    # é è¨­äººè¨­
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹éå¸¸å²å®³çš„ Kubernetes å°ˆå®¶ï¼Œä½ å« KubeWizardã€‚
    ä»¥ä¸‹æ˜¯ä½ çš„å€‹äººè¨­å®š:
    1. ä½ ç²¾é€š Kubernetesã€Dockerã€å®¹å™¨æŠ€è¡“ã€å¾®æœå‹™æ¶æ§‹ç­‰é›²åŸç”ŸæŠ€è¡“ã€‚
    2. ä½ èƒ½å¤ è¨ºæ–· Kubernetes é›†ç¾¤å•é¡Œã€Pod æ•…éšœã€ç¶²çµ¡é…ç½®ã€å­˜å„²å•é¡Œç­‰ã€‚
    3. ä½ æœƒä½¿ç”¨å„ç¨®å·¥å…·ä¾†å¹«åŠ©ç”¨æˆ¶è§£æ±ºå•é¡Œï¼ŒåŒ…æ‹¬æœç´¢ã€æŸ¥è©¢ APIã€åŸ·è¡Œå‘½ä»¤ç­‰ã€‚
    4. ç•¶ç”¨æˆ¶å•ä½ å•é¡Œçš„æ™‚å€™ï¼Œä½ æœƒæœ‰æ¢ç†åœ°åˆ†æå•é¡Œï¼Œæä¾›å°ˆæ¥­çš„è§£æ±ºæ–¹æ¡ˆã€‚
    5. å¦‚æœæ˜¯ä¸­æ–‡å•é¡Œï¼Œå°±ç”¨ç¹é«”ä¸­æ–‡ä¾†ä½œç­”ã€‚
    {who_you_are}
    ä»¥ä¸‹æ˜¯ä½ å¸¸èªªçš„ä¸€äº›å£é ­ç¦ªï¼š
    1. "è®“æˆ‘å€‘ä¸€æ­¥æ­¥ä¾†è¨ºæ–·é€™å€‹å•é¡Œã€‚"
    2. "æ ¹æ“šæ—¥èªŒåˆ†æï¼Œå•é¡Œå¯èƒ½å‡ºåœ¨é€™è£¡ã€‚"
    3. "å»ºè­°å…ˆæª¢æŸ¥ Pod çš„ç‹€æ…‹å’Œäº‹ä»¶ã€‚"
    
    ä»¥ä¸‹æ˜¯ä½ è§£æ±ºå•é¡Œçš„éç¨‹ï¼š
    1. ç•¶åˆæ¬¡å’Œç”¨æˆ¶å°è©±çš„æ™‚å€™ï¼Œä½ æœƒå…ˆäº†è§£ç”¨æˆ¶é‡åˆ°çš„å…·é«”å•é¡Œã€‚
    2. ç•¶ç”¨æˆ¶å¸Œæœ›æŸ¥è©¢å¯¦æ™‚ä¿¡æ¯æ™‚ï¼Œä½ æœƒä½¿ç”¨æœç´¢å·¥å…·ã€‚
    3. ç•¶é‡åˆ°ä¸çŸ¥é“çš„äº‹æƒ…æˆ–è€…ä¸æ˜ç™½çš„æ¦‚å¿µï¼Œä½ æœƒä½¿ç”¨æœç´¢å·¥å…·ä¾†æœç´¢ã€‚
    4. ä½ æœƒæ ¹æ“šç”¨æˆ¶çš„å•é¡Œä½¿ç”¨ä¸åŒçš„åˆé©çš„å·¥å…·ä¾†å›ç­”ã€‚
    5. ä½ æœƒä¿å­˜æ¯ä¸€æ¬¡çš„èŠå¤©è¨˜éŒ„ï¼Œä»¥ä¾¿åœ¨å¾ŒçºŒçš„å°è©±ä¸­ä½¿ç”¨ã€‚
    """

    MEMORY_KEY = "chat_history"

    def __init__(self, user_id: str = "default", llm: BaseChatModel = None, debug_level: Optional[int] = None):
        self.user_id = user_id
        
        # åˆå§‹åŒ–å·¥å…·åˆ—è¡¨ï¼ˆæ ¹æ“šç’°å¢ƒè®Šæ•¸æ±ºå®šæ˜¯å¦åŒ…å«ç®—å‘½å·¥å…·ï¼‰
        self.tools = [
            KubeTool(), 
            KubeToolWithApprove(), 
            human_console_input(), 
            create_search_tool(), 
            RequestsGet(allow_dangerous_requests=True),
        ]
        
        # å¦‚æœå•Ÿç”¨ç®—å‘½å·¥å…·ï¼Œå‰‡æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨
        if ENABLE_FORTUNE_TOOLS:
            print("âœ¨ ç®—å‘½å·¥å…·å·²å•Ÿç”¨")
            self.tools.extend([search, bazi_cesuan, yaoyigua, jiemeng])
        else:
            print("ğŸ”’ ç®—å‘½å·¥å…·å·²é—œé–‰")
        
        # Load AI configuration from environment variables
        if llm is None:
            model = os.getenv("AI_MODEL", "gemini-2.0-flash-exp")
            temperature = float(os.getenv("AI_TEMPERATURE", "0.7"))
            api_key = os.getenv("AI_GOOGLE_API_KEY")
            
            if not api_key:
                raise ValueError("AI_GOOGLE_API_KEY environment variable is required")
            
            self.chatmodel = ChatGoogleGenerativeAI(
                model=model,
                temperature=temperature,
                google_api_key=api_key
            )
        else:
            self.chatmodel = llm
        
        # ä½¿ç”¨æ–°çš„ prompt æ ¼å¼æ”¯æŒç³»çµ±æ¶ˆæ¯å’Œå·¥å…·
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    self.SYSTEM_PROMPT.format(who_you_are=""),
                ),
                MessagesPlaceholder(variable_name=self.MEMORY_KEY),
                (
                    "user",
                    "{input}"
                ),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ],
        )

        # ç²å– Redis èŠå¤©æ­·å²
        self.chat_message_history = self.get_memory()

        # ä½¿ç”¨ ConversationTokenBufferMemory ä¾†ç®¡ç† token é™åˆ¶
        self.memory = ConversationTokenBufferMemory(
            llm=self.chatmodel,
            human_prefix="ç”¨æˆ¶",
            ai_prefix="KubeWizard",
            memory_key=self.MEMORY_KEY,
            output_key="output",
            return_messages=True,
            max_token_limit=2000,
            chat_memory=self.chat_message_history,
        )

        # å‰µå»º agent
        agent = create_openai_tools_agent(
            self.chatmodel,
            tools=self.tools,
            prompt=self.prompt,
        )

        verbose = False
        return_intermediate_steps = False
        if debug_level is None:
            debug_level = int(os.getenv("DEBUG_LEVEL", "1"))

        if debug_level == 1:
            verbose = True
        elif debug_level >= 2:
            verbose = True
            return_intermediate_steps = True

        self.agent = AgentExecutor(
            name=self.name,
            agent=agent,
            memory=self.memory,
            tools=self.tools,
            return_intermediate_steps=return_intermediate_steps,
            handle_parsing_errors=True,
            verbose=verbose,
        )

    def get_memory(self):
        """ä½¿ç”¨ user_id åˆå§‹åŒ– RedisChatMessageHistory"""
        try:
            chat_message_history = RedisChatMessageHistory(
                url=REDIS_URL, 
                session_id=self.user_id, 
                key_prefix="chat_history", 
                ttl=3600
            )
            print(f"chat_message_history for user_id {self.user_id}: {chat_message_history.messages}")

            store_message = chat_message_history.messages
            
            # å¦‚æœæ¶ˆæ¯éå¤šï¼Œé€²è¡Œç¸½çµ
            if len(store_message) > 10:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        (
                            "system",
                            self.SYSTEM_PROMPT + "\né€™æ˜¯ä¸€æ®µä½ å’Œç”¨æˆ¶çš„å°è©±è¨˜æ†¶ï¼Œå°å…¶é€²è¡Œç¸½çµæ‘˜è¦ï¼Œæ‘˜è¦ä½¿ç”¨ç¬¬ä¸€äººç¨±'æˆ‘'ï¼Œä¸¦ä¸”æå–å…¶ä¸­çš„ç”¨æˆ¶é—œéµä¿¡æ¯ã€‚ä»¥å¦‚ä¸‹æ ¼å¼è¿”å›:\n ç¸½çµæ‘˜è¦å…§å®¹ï½œç”¨æˆ¶é—œéµä¿¡æ¯ \n ä¾‹å¦‚ ç”¨æˆ¶å¼µä¸‰å•å€™æˆ‘ï¼Œæˆ‘ç¦®è²Œå›è¦†ï¼Œç„¶å¾Œä»–å•æˆ‘ Kubernetes å•é¡Œï¼Œæˆ‘å¹«ä»–è§£æ±ºäº†ã€‚ï½œå¼µä¸‰"
                        ),
                        ("user", "{input}"),
                    ]
                )
                chain = prompt | self.chatmodel
                summary = chain.invoke({"input": store_message})
                print("summary:", summary)
                chat_message_history.clear()
                chat_message_history.add_message(summary)
                print("ç¸½çµå¾Œï¼š", chat_message_history.messages)
            
            return chat_message_history
        except Exception as e:
            print(f"Redis connection failed: {e}, using in-memory history")
            # å¦‚æœ Redis é€£æ¥å¤±æ•—ï¼Œä½¿ç”¨å…§å­˜æ­·å²
            from langchain.memory import ChatMessageHistory
            return ChatMessageHistory()

    def invoke(self, input: str):
        """åŸ·è¡Œ agent ä¸¦è¿”å›çµæœ"""
        result = self.agent.invoke({
            "input": input,
            "chat_history": self.get_chat_messages(),
        })
        return result

    def get_chat_messages(self):
        """ç²å–èŠå¤©æ­·å²æ¶ˆæ¯"""
        return self.memory.chat_memory.messages
